# Beyond Bayes: Paths Towards Universal Reasoning Systems

- [Abstract](#abstract)
- [Invited Speakers and Panelists](#invited-speakers-and-panelists)
- [Schedule and Planned Activities](#schedule-and-planned-activities)
- [Accepted Posters](#accepted-posters)
- [Organizers](#organizers)
- [Call for Submissions (expired)](#call-for-submissions)

## Abstract
A long-standing objective of AI research has been to discover theories of reasoning that are general: accommodating various forms of knowledge and applicable across a diversity of domains. The last two decades have brought steady advances toward this goal, notably in the form of mature theories of probabilistic and causal inference, and in the explosion of reasoning methods built upon the deep learning revolution. However, these advances have only further exposed gaps in both our basic understanding of reasoning and in limitations in the flexibility and composability of automated reasoning technologies. 

This workshop aims to reinvigorate work on the grand challenge of developing a computational foundation for reasoning in minds, brains, and machines. Goals include:

- Developing a richer shared conceptual and mathematical vocabulary between researchers in neuroscience, cognitive science, Bayesian and causal inference, machine learning, logic, programming languages, and automated reasoning

- Advancing novel knowledge representations and inference algorithms that are qualitatively more general and expressive than the current state of the art, even if they are currently less efficient or less familiar within the ML community

- Exposing and formalizing modes of reasoning that are novel or underexplored within the ML community, and ultimately implementing them in software

- Predicting what lies beyond the current languages and algorithms for reasoning, and pinpointing algorithmic and conceptual roadblocks that must be overcome

## Topics

Specific topics relevant to these larger goals include:

- Cognitive science of formal and intuitive reasoning

- Plausible cognitive neuroscientific foundations for general reasoning systems 

- Machine learning approaches to theorem proving and deductive reasoning

- The semantics of natural language reasoning with large language models

- Design and implementation of probabilistic & differentiable programming languages

- Advances in formal languages and implementations for formalizing mathematics

- Algorithms and representations for causal and counterfactual reasoning

- Causal queries beyond counterfactuals, e.g. actual causality & causal explanation

- Insights from algorithms and representations in modern SAT and SMT solvers

- Unifying semantic and algorithmic frameworks for distinct forms of reasoning

# Invited Speakers and Panelists

*This list is still being finalized and may see further additions or removals*

### [Joshua Tenenbaum](http://web.mit.edu/cocosci/josh.html) *(tentatively confirmed)* 

Joshua Tenenbaum is Professor of Computational Cognitive Science at MIT in the Department of Brain and Cognitive Sciences, the Computer Science and Artificial Intelligence Laboratory (CSAIL) and the Center for Brains, Minds and Machines (CBMM). His long-term goal is to reverse-engineer intelligence in the human mind and brain, and to use these insights to engineer more human-like machine intelligence. In cognitive science, he is best known for developing theories of cognition as probabilistic inference in structured generative models, and applications to concept learning, causal reasoning, language acquisition, visual perception, intuitive physics, and theory of mind. In AI, he and his group have developed widely used models for nonlinear dimensionality reduction, probabilistic programming, and Bayesian unsupervised learning and structure discovery. His current research focuses on common-sense scene understanding and action planning, the development of common sense in infants and young children, and learning through probabilistic program induction and neuro-symbolic program synthesis. His work has been published in many leading journals and recognized with awards at conferences in Cognitive Science, Computer Vision, Neural Information Processing Systems, Reinforcement Learning and Decision Making, and Robotics. He is the recipient of the Distinguished Scientific Award for Early Career Contributions in Psychology from the American Psychological Association (2008), the Troland Research Award from the National Academy of Sciences (2011), the Howard Crosby Warren Medal from the Society of Experimental Psychologists (2016), the R&D Magazine Innovator of the Year award (2018), and a MacArthur Fellowship (2019). He is a fellow of the Cognitive Science Society, the Society for Experimental Psychologists, and a member of the American Academy of Arts and Sciences.

### [Guy Van den Broeck](http://web.cs.ucla.edu/~guyvdb/)
<img src="https://user-images.githubusercontent.com/2993689/164081243-dcc8b5bb-a267-4688-9a2f-37a740e53938.jpg" width="200" />
Guy Van den Broeck is an Associate Professor and Samueli Fellow at UCLA, in the Computer Science Department, where he directs the Statistical and Relational Artificial Intelligence (StarAI) lab. His research interests are in Machine Learning, Knowledge Representation and Reasoning, and Artificial Intelligence in general. His papers have been recognized with awards from key conferences such as AAAI, UAI, KR, OOPSLA, and ILP. Guy is the recipient of an NSF CAREER award, a Sloan Fellowship, and the IJCAI-19 Computers and Thought Award. 

### [Kimberly Stachenfeld](https://neurokim.com/)
<img src="https://user-images.githubusercontent.com/2993689/164081330-027fe397-eb57-46bc-8591-8e410aca2482.jpg" width="200" />
Kimberly Stachenfeld is a Senior Research Scientist at DeepMind. She received her PhD in Computational Neuroscience from Princeton University in 2018. Before that, she received her Bachelors from Tufts University in 2013, where she majored in Chemical & Biological Engineering and Mathematics. Her research is at the interface of Neuroscience and Machine Learning, focusing on graph-based computations for efficient learning and planning in brains and in machines. 

### [Thomas Icard](https://web.stanford.edu/~icard/)
<img src="https://user-images.githubusercontent.com/2993689/164081369-a0009bb1-6548-4086-9e84-f4f61e29f48c.jpeg" width="200" />
Thomas Icard is an Associate Professor of Philosophy and (by courtesy) of Computer Science at Stanford University. Thomas works at the intersection of philosophy, cognitive science, and computer science, especially on topics that sit near the boundary between the normative (how we ought to think and act) and the descriptive (how we in fact do think and act). Much of his research concerns the theory and application of logic, probability, and causal modeling and inference. Some current topics of interest include explanation, the quantitative/qualitative interface, and reasoning with limited resources.

### [Tyler Bonnen](https://neuroscience.stanford.edu/people/tyler-bonnen)
<img src="https://neuroscience.stanford.edu/sites/g/files/sbiybj1576/f/styles/large-square/public/r_2eo6alvtpkc3ed6_ok.png" width="200" />
Tyler is a PhD student in the Department of Psychology at Stanford University. After transferring from Miami-Dade Community College, tyler studied chemistry and comparative literature at Columbia University. He went on to research fellowships in the Max-Planck Institute in Leipzig, then in the Department of Brain and Cognitive Sciences at MIT before coming to Stanford. In his current work, co-advised by Anthony Wagner and Daniel Yamins, tyler uses biologically plausible computational models, neural data, and animal behavior in order to formalize the relationship between perception and memory.

### [Ishita Dasgupta](https://ishita-dg.github.io/)
<img src="https://user-images.githubusercontent.com/47085068/165039415-522f87fe-0049-4369-9a3b-619f4b3b1555.jpeg" width="200" />
Ishita is a Research Scientist at DeepMind New York City. She was previously a postdoctoral researcher at Princeton University in the Departments of Psychology and Computer Science, working in the Computational Cognitive Science Lab with Prof. Tom Griffiths. She received her PhD from the Department of Physics at Harvard University in 2020, working in the Computational Cognitive Neuroscience Lab with Prof. Sam Gershman. Her research is at the intersection of computational cognitive science and machine learning. Ishita uses advances in machine learning to build new models of human reasoning, applies cognitive science approaches toward understanding black-box AI systems, and combines these insights to build better, more human-like artificial intelligence.

### [Jan-Willem van de Meent](https://jwvdm.github.io/)
<img src="https://user-images.githubusercontent.com/47085068/164697690-f796fb33-71cc-4e4f-b4ef-7546a40b5264.jpg" width="200" />
Dr. Jan-Willem van de Meent is an Associate Professor (Universitair Hoofddocent) at the University of Amsterdam. He co-directs the [AMLab](https://amlab.science.uva.nl/) with Max Welling and co-directs the [Uva Bosch Delta Lab](https://ivi.fnwi.uva.nl/uvaboschdeltalab/) with Theo Gevers. He also holds a position as an Assistant Professor at Northeastern University, where he is currently on leave. Prior to becoming faculty at Northeastern, he held a postdoctoral position with Frank Wood at Oxford, as well as a postdoctoral position with Chris Wiggins and Ruben Gonzalez at Columbia University. He carried out his PhD research in biophysics at Leiden and Cambridge with Wim van Saarloos and Ray Goldstein.

Jan-Willem van de Meent’s group develops models for artificial intelligence by combining probabilistic programming and deep learning. A major theme in this work is understanding which inductive biases can enable models to generalize from limited data. Inductive biases can take the form of a simulator that incorporates knowledge of an underlying physical system, causal structure, or symmetries of the underlying domain. At a technical level, his group develops inference methods, along with corresponding language abstractions to make these methods more modular and composable. To guide this technical work, his group collaborates extensively to develop models for robotics, NLP, healthcare, and the physical sciences.

Jan-Willem van de Meent is one of the creators of [Anglican](https://probprog.github.io/anglican/), a probabilistic language based on Clojure. His group currently develops [Probabilistic Torch](https://github.com/probtorch/probtorch), a library for deep generative models that extends PyTorch. He is an author on a forthcoming book on probabilistic programming, a draft of which is available on arXiv. He is a co-chair of the international conference on probabilistic programming ([PROBPROG](https://probprog.cc/)). He was the recipient of an NWO Rubicon Fellowship and is a current recipient of the NSF CAREER award.

### [Charles Sutton](https://homepages.inf.ed.ac.uk/csutton/)
<img src="https://homepages.inf.ed.ac.uk/csutton/images/charles3.jpg" width="200" />
Charles Sutton is a Research Scientist at Google Brain and a Reader (equivalent to Associate Professor: http://bit.ly/1W9UhqT) in Machine Learning at the University of Edinburgh. He has published over 50 papers in probabilistic machine learning and deep learning, motivated by the demands of a broad range of applications, including natural language processing (NLP), analysis of computer systems, sustainable energy, data analysis, and software engineering. His work in machine learning for software engineering has won two ACM Distinguished Paper Awards.

# Schedule and Planned Activities
The workshop will consist of an opening keynote presentation and three sessions, each broken up by 30 minute ICML plenary breaks. Discussion will be encouraged at all sessions. Invited talks will be 20 minutes followed by 25 minute panel discussions, in which the audience is welcome to participate in the Q+A. Contributed talks will be rapid, 5 minute presentations; attendees may use the ICML break times immediately following these presentations to meet the researchers and ask any questions. 

The workshop will be held at the Baltimore Convention Center in Ballroom 2 (Level 400) on Friday, July 22nd, 2022.	

## Schedule

**8:45 AM - 9:00 AM**: Doors Open & Welcome (Opening Remarks) 

**9:00 AM - 9:45 AM**: Opening Keynote: Cognitive Science of Reasoning

- Speaker: Thomas Icard, Associate Professor of Philosophy and (by courtesy) of Computer Science at Stanford University
- Moderator/MC: Zenna Tavares

**9:45 AM - 10:00 AM**: Contributed Spotlight Talks: Part 1 

- Talk 1 (P23): Language Model Cascades (David Dohan, Winnie Xu)
- Talk 2 (P08): Map Induction: Compositional Spatial Submap Learning for Efficient Exploration in Novel Environments (Sugandha Sharma)
- Talk 3 (P18): Abstract Interpretation for Generalized Heuristic Search in Model-Based Planning (Tan Zhi-Xuan, Vikash K. Mansinghka)

**10:00 AM - 10:30 AM**: ICML Plenary Break 

**10:30 AM - 11:35 PM**: Session 1: New Reasoning Problems and Modes of Reasoning (Talks & Panel Discussion)

- Moderator/MC: Robert Ness
- Talk 1 (20 min): Joshua Tenenbaum, Professor of Computational Cognitive Science at MIT in the Department of Brain and Cognitive Sciences, the Computer Science and Artificial Intelligence Laboratory (CSAIL) and the Center for Brains, Minds and Machines (CBMM)
- Talk 2 (20 min): Charles Sutton, Research Scientist, Google AI; Associate Professor, School of Informatics, University of Edinburgh; and Fellow, The Alan Turing Institute
- Q+A and Group Panel Discussion (25 min) - Thomas Icard will also join this panel discussion

**11:35 AM - 12:00 PM**: Contributed Spotlight Talks: Part 2

- Talk 4 (P10): Combining Functional and Automata Synthesis to Discover Causal Reactive Programs (Ria Das)
- Talk 5 (P26): Biological Mechanisms for Learning Predictive Models of the World and Generating Flexible Predictions (Ching Fang)
- Talk 6 (P02): Designing Perceptual Puzzles by Differentiating Probabilistic Programs (Kartik Chandra)
- Talk 7 (P14): Logical Activation Functions (Scott Lowe)

**12:00 PM - 1:30 PM**: ICML Plenary Lunch Break

**1:30 PM - 3:00 PM**: Session 2: Reasoning in Brains vs Machines (Talks & Panel Discussion)

- Moderator/MC: Emily Mackevicius
- Talk 1 (20 min): Kim Stachenfeld, Senior Research Scientist, DeepMind
- Talk 2 (20 min): Tyler Bonnen, PhD Student, Department of Psychology, Stanford University
- Talk 3 (20 min): Ishita Dasgupta, Research Scientist at DeepMind
- Q+A and Group Panel Discussion (30 min)

**3:00 PM - 3:30 PM**: ICML Plenary Break

**3:30 PM - 4:55pm**: Session 3: New Computational Technologies for Reasoning (Talks & Panel Discussion)

- Moderator/MC: Armando Solar-Lezama
- Talk 1 (20 min): Guy Van den Broeck, Associate Professor and Samueli Fellow, Computer Science Department, UCLA
- Talk 2 (20 min): Jan-Willem van de Meent, Associate Professor (Universitair Hoofddocent), University of Amsterdam
- Talk 3: Speaker TBD
- Q+A and Group Panel Discussion (25 min)

**4:55 PM - 5:00 PM**: Closing Remarks

**5:00 PM - 6:00 PM**: Poster Session (In Person Only)

# Accepted Posters

[Google Drive PDFs](https://drive.google.com/drive/folders/1EKuXBIEN-3Y3ebgaWahJhRcMmJWh9DCj)

P01: Maximum Entropy Function Learning. Authors: Simon Segert, Jonathan Cohen. [paper](https://drive.google.com/file/d/1lvQq2xHUHDAhFQML52bpTJu9xKBbCtGO/view?usp=sharing)

P02: Designing Perceptual Puzzles by Differentiating Probabilistic Programs. Authors: Kartik Chandra, Tzu-Mao Li, Joshua B. Tenenbaum, Jonathan Ragan-Kelley. [paper](https://drive.google.com/file/d/1rxoGIZvqVV98j-u-92cblCxz1iYT5ary/view?usp=sharing)

P03: Interoception as Modeling, Allostasis as Control. Authors: Eli Zachary Sennesh, Jordan Theriault, Dana Brooks, Jan-Willem van de Meent, Lisa Feldman Barrett, Karen Quigley. [paper](https://drive.google.com/file/d/1Km3La2npEetTB1oaDlKBjjWTsffmQIMl/view?usp=sharing)

P04: People Construct Simplified Mental Representations to Plan. Authors: Mark K Ho, David Abel, Carlos G. Correa, Michael Littman, Jonathan Cohen, Thomas L. Griffiths. [paper](https://www.nature.com/articles/s41586-022-04743-9.epdf?sharing_token=05ou7pT3QJfIbxrHZvRMC9RgN0jAjWel9jnR3ZoTv0Oomhhb6zGajHz2tGh1l0cpwVoKG3R0H0EUwGENmWFwLqJzWlnnIZJ0z23004gb70eBSASDgizZ5af_1rk6SMXTMoK3GeZhhDo9ndibk8zqH33DvIFZh2sxyVY1P7ngPU0%3D)

P05: Using Language and Programs to Instill Human Inductive Biases in Machines. Authors: Sreejan Kumar, Carlos G. Correa, Ishita Dasgupta, Raja Marjieh, Michael Hu, Robert D. Hawkins, Nathaniel Daw, Jonathan Cohen, Karthik R Narasimhan, Thomas L. Griffiths. [paper](https://drive.google.com/file/d/1fZBF0uzTt2ozp-SaXpo-iUSDTwzEKBP6/view?usp=sharing)

P06: Automatic Inference with Pseudo-Marginal Hamiltonian Monte Carlo. Authors: Jinlin Lai, Daniel Sheldon. [paper](https://drive.google.com/file/d/1dIZ3CYLsdtCqoMEALtJqB6qGqmikbIph/view?usp=sharing)

P07: MoCa: Cognitive Scaffolding for Language Models in Causal and Moral Judgment Tasks. Authors: Allen Nie, Atharva Amdekar, Christopher J Piech, Tatsunori Hashimoto, Tobias Gerstenberg. [paper](https://drive.google.com/file/d/1PNCqgY7rzJjg22CsicQ0qwPtaTgjrhVV/view?usp=sharing)

P08: Map Induction: Compositional Spatial Submap Learning for Efficient Exploration in Novel Environments. Authors: Sugandha Sharma, Aidan Curtis, Marta Kryven, Joshua B. Tenenbaum, Ila R Fiete. [paper](https://drive.google.com/file/d/1MxSHb9etiSVwtI3toyFpi1PxprLnAhh_/view?usp=sharing)

P09: Towards a Neuroscience of "Stories”: Metric Space Learning in the Hippocampus. Authors: Zhenrui Liao, Attila Losonczy. [paper](https://drive.google.com/file/d/1C5YovpalkopgXVjPFKluP_9cTC4VAj6m/view)

P10: Combining Functional and Automata Synthesis to Discover Causal Reactive Programs. Authors: Ria Das, Joshua B. Tenenbaum, Armando Solar-Lezama, Zenna Tavares
 
P11: MetaCOG: Learning a Meta-cognition to Recover what Objects are Actually There. Authors: Marlene Berke, Zhangir Azerbayev, Mario Belledonne, Zenna Tavares, Julian Jara-Ettinger. [paper](https://drive.google.com/file/d/10IA-bMrjZLiBv-IzhU5dHjGzcuC1a4UB/view?usp=sharing)

P12: Desiderata for Abstraction. Authors: Simon Alford, Zenna Tavares, Kevin Ellis. [paper](https://drive.google.com/file/d/1hGLuqIAd0A-E6xiIkqOzXGQleMd6Sozk/view?usp=sharing)

P13: Estimating Categorical Counterfactuals via Deep Twin Networks. Authors: Athanasios Vlontzos, Bernhard Kainz, Ciarán Mark Gilligan-Lee. [paper](https://drive.google.com/file/d/1Nsl-zoR7ZXIjTpCvRuaAc_5VlqRXInML/view?usp=sharing)

P14: Logical Activation Functions: Logit-space Equivalents of Probabilistic Boolean Operators. Authors: Scott C Lowe, Robert Earle, Jason d'Eon, Thomas Trappenberg, Sageev Oore. [paper](https://drive.google.com/file/d/1yMquez8KwMiN1yQ0Vh8eN2s-gFTk8Yqf/view?usp=sharing)

P15: Bias of Causal Identification using Non-IID Data. Authors: Chi Zhang, Karthika Mohan, Judea Pearl. [paper](https://drive.google.com/file/d/18uKWkpYyY3euMxFq76xRa_Kdq6RwMCbc/view)

P16: Bayesian Reasoning with Trained Neural Networks. Authors: Jakob Knollmüller, Torsten Ensslin. [paper](https://drive.google.com/file/d/1KxspuUNzZ0-D043_BWz2WYsn-d7uWDXt/view)

P17: Correcting Model Bias with Sparse Implicit Processes. Authors: Simon Rodriguez Santana, Luis A. Ortega, Daniel Hernández-Lobato, Bryan Zaldivar. [paper](https://drive.google.com/file/d/1Y6jr68a8cxI5nfdSHfnGpjMIOjHcHQQz/view)

P18: Abstract Interpretation for Generalized Heuristic Search in Model-Based Planning. Authors: Tan Zhi-Xuan, Joshua B. Tenenbaum, Vikash Mansinghka. [paper](https://drive.google.com/file/d/1zKtGzy6dLh49eHw2uqs5kELv7h8nfaTX/view)

P19: Logical Satisfiability of Counterfactuals for Faithful Explanations in NLI. Authors: Suzanna Sia, Anton Belyy, Amjad Almahairi, Madian Khabsa, Luke Zettlemoyer, Lambert Mathias. [paper](https://arxiv.org/pdf/2205.12469.pdf)

P20: Learning to Reason about and to Act on Cascading Events. Authors: Yuval Atzmon, Eli Meirom, Shie Mannor, Gal Chechik. [paper](https://arxiv.org/pdf/2202.01108.pdf)

P21: Reverse-Mode Automatic Differentiation and Optimization of GPU Kernels via Enzyme. Authors: William S. Moses, Valentin Churavy, Ludger Paehler, Jan Hückelheim, Sri Hari Krishna Narayanan, Michel Schanen, Johannes Doerfert. [paper](https://drive.google.com/file/d/1db-NaNL6sjpIrt2CMC7qTLhOonx67wLl/view) - originally published at SC '21

P22: Type Theory for Inference and Learning in Minds and Machines. Author: Felix Anthony Sosa, Tomer D. Ullman. [paper](https://drive.google.com/file/d/1PMeGhoOaPFn7spffKgqLldZQB9_VUlNR/view)

P23: Language Model Cascades. Authors: David Dohan, Aitor Lewkowycz, Jacob Austin, Winnie Xu, Yuhuai Wu, David Bieber, Raphael Gontijo-Lopes, Henryk Michalewski, Rif A. Saurous, Jascha Sohl-Dickstein, Kevin Patrick Murphy, Charles Sutton. [paper](https://drive.google.com/file/d/1pOFwRDEBvo9QtJgrJy-l7Lna7bfBPgU6/view)

P24: Unifying Generative Models with GFlowNets. Authors: Dinghuai Zhang, Ricky T. Q. Chen, Nikolay Malkin, Yoshua Bengio. [paper](https://drive.google.com/file/d/1ejC6vxzMpN8krSJ5XYpqNXwZTVroAVXj/view)

P25: Proving Theorems using Incremental Learning and Hindsight Experience Replay. Authors: Eser Aygün, Laurent Orseau, Ankit Anand, Xavier Glorot, Stephen Marcus McAleer, Vlad Firoiu, Lei M Zhang, Doina Precup, Shibl Mourad. [paper](https://drive.google.com/file/d/1xGOzP7fxPidOMPeC6GA0TrZsohDW5Y2n/view)

P26: Biological Mechanisms for Learning Predictive Models of the World and Generating Flexible Predictions. Authors: Ching Fang, Dmitriy Aronov, Larry Abbott, Emily L Mackevicius. [paper](https://drive.google.com/file/d/1464hwZB6kakF5o_Z2LcId-1ikC_mos4U/view)

P27: Explanatory Paradigms in Neural Networks. Authors: Mohit Prabhushankar, Ghassan AlRegib. [paper](https://drive.google.com/file/d/1OL9-Xodd3AXvGsOoSELvf0IhvHkiWOSh/view)

P28: On the Generalization and Adaption Performance of Causal Models. Authors: Nino Scherrer, Anirudh Goyal, Stefan Bauer, Yoshua Bengio, Nan Rosemary Ke. 

P29: Predicting Human Similarity Judgments Using Large Language Models. Authors: Raja Marjieh, Ilia Sucholutsky, Theodore Sumers, Nori Jacoby, Thomas L. Griffiths. [paper](https://drive.google.com/file/d/1gyVi6OiBdT9q5uUXedsU90JvDYb_Zxn3/view)

P30: Meta-Learning Real-Time Bayesian AutoML For Small Tabular Data. Authors: Noah Hollmann, Samuel Müller, Katharina Eggensperger, Frank Hutter. [paper](https://drive.google.com/file/d/1gIvDRkdMtyW_CvgYNgtRN-3_BFibditz/view)

P31: Can Humans Do Less-Than-One-Shot Learning? Authors: Maya Malaviya, Ilia Sucholutsky, Kerem Oktar, Thomas L. Griffiths. [paper](https://drive.google.com/file/d/1Z8f_cKv_Wi4sQvdKcs4fCWLDFO28btDI/view)

P32: Collapsed Inference for Bayesian Deep Learning. Authors: Zhe Zeng, Guy Van den Broeck. [paper](https://drive.google.com/file/d/1Y84iUJAGGEbmqgPiwAxb_v_5jwWl9CsS/view)

P33: ViRel: Unsupervised Visual Relations Discovery with Graph-level Analogy. Authors: Daniel Zeng, Tailin Wu, Jure Leskovec. [paper](https://drive.google.com/file/d/1zfH1TRddImSrjuWEUHvjK1UGDh6yAibL/view)

P34: ZeroC: A Neuro-Symbolic Model for Zero-shot Concept Recognition and Acquisition at Inference Time. Authors: Tailin Wu, Megan Tjandrasuwita, Zhengxuan Wu, Xuelin Yang, Kevin Liu, Rok Sosic, Jure Leskovec. [paper](https://drive.google.com/file/d/1Jd0Y7cxWQ8tZQ85FAqMYLpqID86gmuZF/view)

P35: Hybrid AI Integration Using Implicit Representations With Scruff. Authors: Avi Pfeffer, Michael Harradon, Sanja Cvijic, Joseph Campolongo. [paper](https://drive.google.com/file/d/1vdwRZIpf0S4m4g9um8BtFUp2qJGj4COY/view) 

P36: Large Language Models are Zero-Shot Reasoners. Authors: Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, Yusuke Iwasawa. [paper](https://drive.google.com/file/d/1eTCOjqMFPFIDG4IdjAPPvY2bFZdQrtR3/view)

P37: Structured, Flexible, and Robust: Benchmarking and Improving Large Language Models Towards More Human-like Behavior in Out-of-Distribution Reasoning Tasks. Authors: Katherine M. Collins, Catherine Wong, Jiahei Feng, Megan Wei, Joshua B. Tenenbaum. [paper](https://drive.google.com/file/d/1Kje5crEaDLjvWwb3i5yHyUss8IbRihWY/view)

# Organizers

### [Nada Amin](https://namin.seas.harvard.edu/)
Nada Amin is an assistant professor of computer science at Harvard SEAS. Previously, she was a University Lecturer in Programming Languages at the University of Cambridge, and a member of the team behind the Scala programming language at EPFL. She is broadly interested in programming languages, and the intersection of programming languages and artificial intelligence. She has co-organized the Scala, Scheme, miniKanren and TyDe (Type-driven Development) workshops, and has served on the program committee of POPL, FLOPS, OOPSLA, UAI among others.

### [Eli Bingham](https://pyro.ai/)
Eli is a Machine Learning Fellow at the Broad Institute of MIT and Harvard's Data Sciences Platform, where he develops machine learning methods and software for biomedical research applications, and was previously a senior research scientist at Uber AI Labs. His research at the intersection of programming languages and AI focuses on developing general methods for approximate Bayesian inference suited for new and previously inaccessible problems, and on democratization of those methods through the Pyro probabilistic programming language, of which he is a co-creator and core developer. He has served as a program committee member of scientific workshops including HOPE and LAFI, and has also organized and led a number of public and private workshops and tutorials for current and prospective Pyro users.

### [Nan Rosemary Ke](https://nke001.github.io/)
Rosemary is a research scientist at Deepmind. Previously, she was a PhD student at Mila, advised by Yoshua Bengio and Chris Pal. Her research centers around developing novel machine learning algorithms that can generalize well to changing environments. Her research focuses on two key ingredients: credit assignment and causal learning. These two ingredients flow into and reinforce each other: appropriate credit assignment can help a model refine itself only at relevant causal variables, while a model that comprehends causality sufficiently well can reason about the connections between causal variables and the effect of intervening on them. She has co-organized a conference, 6 workshops and 3 challenges. These are the conference on causal learning and reasoning (CLeaR) 2022, the “inductive biases, invariances and generalization in reinforcement learning” workshop at ICML 2020, “causal learning for decision making” workshop at ICLR2020, “efficient credit assignment workshop” at ICML 2018, “reproducibility in machine learning” at ICML 2017, ICML 2018 and ICLR 2019), the “Real robot challenge” at NeurIPS 2021 and the ICLR reproducibility challenge at ICLR 2018 and ICLR 2019. 

### [John Krakauer](http://www.blam-lab.org/)
Dr. John Krakauer is John C. Malone Professor, Professor of Neurology, Neuroscience, and Physical Medicine and Rehabilitation, Director of the Brain, Learning, Animation, and Movement Lab at The Johns Hopkins University School of Medicine, and External Professor at the Santa Fe Institute. His areas of research interest include experimental and computational studies of motor control and motor learning in humans, motor recovery and rehabilitation after stroke, and philosophy of mind. He has organized numerous workshops and scientific meetings, recently including The Learning Salon.

### [Emily Mackevicius](https://emackev.github.io/)
Emily Mackevicius is currently a postdoctoral neuroscientist at Columbia University in the Aronov lab. Previously, she completed her Ph.D. in neuroscience at MIT in the Fee lab. Her research investigates how the brain learns new information in the context of prior knowledge.  Her work involves both experiments (recording neurons in birds performing naturalistic memory behaviors) and theory/computation (modeling how neural circuits self-organize, and developing a sequence-detection method, seqNMF).  She has been involved in organizing a variety of scientific meetings, including founding an ongoing tutorial series on computational topics at MIT's Brain and Cognitive Sciences Department, and TAing Woods Hole summer courses ("Methods in Computational Neuroscience", and "Brains, Minds, and Machines"). 

### [Robert Osazuwa Ness](https://scholar.google.com/citations?user=8gWTOBAAAAAJ&hl=en&oi=ao)
Robert Osazuwa Ness is a Senior Research Scientist at Microsoft Research in Redmond, WA. He is a tech lead on MSR's Societal Resilience team. Robert's research aims to automate human reasoning by enabling experts to program domain knowledge into learning algorithms to achieve predictive capabilities not possible from data alone. He leads the development of MSR's causal machine learning platform and conducts research into probabilistic models for advanced causal reasoning. Before joining MSR, he worked as a machine learning research engineer in various startups. He attended graduate school at both Johns Hopkins SAIS and Purdue University. He received his Ph.D. in Statistics from Purdue, where his dissertation research focused on Bayesian active learning models for causal discovery.

### [Talia Ringer](https://dependenttyp.es/)
Talia Ringer is an assistant professor at the University of Illinois at Urbana-Champaign. Her work focuses on tools that make it easier to develop and maintain systems verified using proof assistants. Toward that end, she loves to use the whole toolbox—everything from dependent type theory to program transformations to neural proof synthesis—all in service of real humans verifying real systems. Prior to Illinois, she earned her PhD in 2021 from the University of Washington. She also has experience in industry. She has served the community in many capacities, including as founder and chair of the SIGPLAN long-term mentoring committee (SIGPLAN-M), co-chair of PLMW at ICFP 2020, hybridization co-chair of SPLASH 2021, co-organizer of the Coq Workshop 2022, and program committee member for PLDI, ITP, TYPES, CAV, CoqPL, HATRA, and AIPLANS.

### [Armando Solar-Lezama](http://people.csail.mit.edu/asolar/)
Armando Solar-Lezama is a Professor in the Department of Electrical Engineering and Computer science and associate director of the Computer Science and Artificial Intelligence Lab at MIT. His background is in programming languages, where he is best known for his seminal work on program synthesis. More recently, he has been working at the intersection of programming languages and machine learning, exploring learning techniques that combine the formal guarantees of program synthesis with the expressiveness of traditional machine learning.  He has co-organized a number of workshops including the Workshop on Computer Assisted Programming (CAP) at NeurIPS 2020 and the workshop on Machine Learning and Programming Languages (MAPL) at PLDI 2019. 

### [Zenna Tavares](http://www.zenna.org/)
Zenna Tavares is the inaugural Innovation Scholar in Columbia University’s Zuckerman Mind Brain Behavior Institute, and Associate Research Scientist in the Data Science Institute. Zenna’s research aims to understand how humans reason, that is, how they come to derive knowledge from observing and interacting with the world. He also constructs computational and statistical tools that help advance his work on causal reasoning, probabilistic programming, and other areas. Prior to Columbia University, he was at MIT, where he received a PhD in Cognitive Science and Statistics and was a Postdoctoral Research researcher in the Computer Science Artificial Intelligence Lab (CSAIL). Zenna has co-organized a number of workshops, including DBAI at Neurips 2021, and OOD Generalization at Neurips 2021, and served on the program committee for UAI, ICML, Neurips, ICLR, and LAFI (POPL).


# Call for Submissions
 
We seek submissions related to any of the topics in the overview, especially ongoing or preliminary work that bridges gaps between topics, and work that might be unfamiliar to the broader ICML community. Submissions will be lightly reviewed for relevance and clarity. All accepted submissions will be presented as posters at a poster session, and a subset will also be selected for oral presentation as contributed talks.  Talks will be selected to generate interesting discussions -- speculative/perspective abstracts welcome.

**Submissions are due in [OpenReview](https://openreview.net/group?id=ICML.cc/2022/Workshop/Beyond_Bayes) before midnight AoE on June 7, 2022 (extended deadline)** and may take one of two forms:

1. **Extended Abstracts**: Authors may submit ongoing or preliminary work in the form of an extended abstract of 2-4 pages (excluding references or appendices, preference for shorter abstracts) for consideration for a poster presentation. Submissions should be anonymized and formatted in the [ICML style](https://icml.cc/Conferences/2022/CallForPapers). Abstracts are non-archival, but will be publicly posted on the workshop website if accepted.
2. **Syndicated Submissions**: Authors may also submit recent work that has been accepted for publication in another venue within the last 12 months of the deadline for consideration for a poster presentation. To encourage broad participation, preference will be given to work on topics that might be less familiar to the ICML community. Syndicated submissions can be in their original format, have no length requirement, and do not need to be anonymized. They are also non-archival, but will be posted publicly on the workshop website if accepted.

To stimulate discussion and interaction, poster presentations will be entirely in-person absent any further changes from the ICML conference chairs. Some need-based funding for travel and registration for speakers and poster presenters may be available from the workshop’s sponsors.
